{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"0PsTttc\",\n",
      "  \"cluster_name\" : \"foodmap-cluster\",\n",
      "  \"cluster_uuid\" : \"2YIbFkvuRqaPWhEjuEYnwA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"5.4.0\",\n",
      "    \"build_hash\" : \"780f8c4\",\n",
      "    \"build_date\" : \"2017-04-28T17:43:27.229Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"6.5.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('http://localhost:8055')\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Elasticsearch([{u'host': 'localhost', u'scheme': 'http'}])>\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(['http://localhost'], port=8055)\n",
    "print es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158986\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2812', u'_source': {u'category': u'baby_back_ribs', u'date': 20170319, u'datetime_date': u'2017-03-19', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2813', u'_source': {u'category': u'baby_back_ribs', u'date': 20170404, u'datetime_date': u'2017-04-04', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2819', u'_source': {u'category': u'baby_back_ribs', u'date': 20170611, u'datetime_date': u'2017-06-11', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2820', u'_source': {u'category': u'baby_back_ribs', u'date': 20170626, u'datetime_date': u'2017-06-26', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2821', u'_source': {u'category': u'baby_back_ribs', u'date': 20170629, u'datetime_date': u'2017-06-29', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2823', u'_source': {u'category': u'baby_back_ribs', u'date': 20170708, u'datetime_date': u'2017-07-08', u'count': 1, u'country': u'saudi arabia'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2828', u'_source': {u'category': u'baby_back_ribs', u'date': 20170703, u'datetime_date': u'2017-07-03', u'count': 2, u'country': u'argentina'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2830', u'_source': {u'category': u'baby_back_ribs', u'date': 20170320, u'datetime_date': u'2017-03-20', u'count': 1, u'country': u'argentina'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2835', u'_source': {u'category': u'baby_back_ribs', u'date': 20170410, u'datetime_date': u'2017-04-10', u'count': 1, u'country': u'argentina'}, u'_index': u'agg'}\n",
      "{u'_score': 1.0, u'_type': u'count', u'_id': u'2838', u'_source': {u'category': u'baby_back_ribs', u'date': 20170522, u'datetime_date': u'2017-05-22', u'count': 1, u'country': u'argentina'}, u'_index': u'agg'}\n"
     ]
    }
   ],
   "source": [
    "result = es.search(index='agg', doc_type='count', size=1000000, body={\n",
    "    'query': {\n",
    "        \"match_all\" : {}\n",
    "    }\n",
    "})\n",
    "\n",
    "lst = [x for x in result['hits']['hits']]\n",
    "print len(lst)\n",
    "for l in lst[:10]:\n",
    "    print l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "# the interval is currently implicit\n",
    "dateBegin = 0\n",
    "dateEnd = 20180400\n",
    "category = 'sushi'\n",
    "# country = 'united states'\n",
    "country = 'italy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query by food category - Fig. 2b,c Map/Report of Trends/Popularity\n",
    "def query_by_category(category):\n",
    "    result = es.search(index='agg', doc_type='count', size=100000, body={\n",
    "        \"query\": {        \n",
    "            \"bool\" : {                \n",
    "                \"filter\": [\n",
    "                    {\"term\" : { \"category.keyword\" : category }},\n",
    "                    {\"range\": {            \n",
    "                        \"date\" : {            \n",
    "                            \"gte\" : dateBegin,\n",
    "                            \"lte\" : dateEnd\n",
    "                        }            \n",
    "                    }}\n",
    "                ]            \n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query by food category - Fig. 2a Report of Trends/Popularity\n",
    "def query_by_country(country):\n",
    "    result = es.search(index='agg', doc_type='count', size=100000, body={\n",
    "        \"query\": {        \n",
    "            \"bool\" : {                \n",
    "                \"filter\": [\n",
    "                    {\"term\" : { \"country.keyword\" : country }},\n",
    "                    {\"range\": {            \n",
    "                        \"date\" : {            \n",
    "                            \"gte\" : dateBegin,\n",
    "                            \"lte\" : dateEnd\n",
    "                        }            \n",
    "                    }}\n",
    "                ]            \n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends / Popularity based on query_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get trends \n",
    "result = query_by_category(category)\n",
    "\n",
    "\n",
    "def get_total_freq_country(country):\n",
    "    result_country = query_by_country(country)\n",
    "    total = 0\n",
    "    for x in result_country['hits']['hits']:\n",
    "        print x\n",
    "        total += x[\"_source\"]['count']\n",
    "    return total\n",
    "\n",
    "def get_trend_per_country(query_result, analysis_type=\"trend\", interval=None):\n",
    "    country_dict = defaultdict(list)\n",
    "    for x in query_result['hits']['hits']:\n",
    "        country_dict[x[\"_source\"]['country']].append(tuple((x[\"_source\"]['date'],x[\"_source\"]['count'])))\n",
    "    \n",
    "    country_trend = defaultdict(float)\n",
    "    for country, values in country_dict.iteritems():\n",
    "        # sort list by date ascending\n",
    "        sorted_counts = sorted(values, key=lambda x: x[0])\n",
    "        # we refer to the current day in the interval\n",
    "        if analysis_type == \"frequency\":\n",
    "            country_trend[country] = sum([y for x,y in sorted_counts])\n",
    "        elif analysis_type == \"relative_frequency\":\n",
    "            total_sum = get_total_freq_country(country)\n",
    "            country_trend[country] = sum([y for x,y in sorted_counts])/(1.0*total_sum)\n",
    "            print country, sum([y for x,y in sorted_counts]), total_sum\n",
    "        elif analysis_type == \"popularity\":\n",
    "            if interval is None:\n",
    "                y = [y for x, y in sorted_counts]\n",
    "            else:\n",
    "                new_intervals = split_intervat_in_buckets(interval, sorted_counts)\n",
    "                print new_intervals\n",
    "                y = [y for x, y in new_intervals]\n",
    "            country_trend[country] = stats.zscore(y)[-1] \n",
    "        elif analysis_type == \"trend\":\n",
    "            if interval is None:\n",
    "                y = [y for x, y in sorted_counts]\n",
    "            else:\n",
    "                new_intervals = split_intervat_in_buckets(interval, sorted_counts)\n",
    "                print new_intervals\n",
    "                y = [y for x, y in new_intervals]\n",
    "            x = np.arange(len(y))\n",
    "            try:\n",
    "                regression = np.polyfit(x, y, 1)\n",
    "                country_trend[country] = regression[0]\n",
    "            except:\n",
    "                # the fir doesn't work for this example: np.polyfit(np.array([0]), np.array([1]), 1)\n",
    "                # some versions of numpy return nan, as should we\n",
    "                country_trend[country] = np.nan\n",
    "            \n",
    "    return country_trend\n",
    "\n",
    "newd = get_trend_per_country(result, analysis_type=\"relative_frequency\")\n",
    "# newd = get_category_trend_per_country(result, analysis_type=\"popularity\")\n",
    "print [(x,y) for x,y in newd.iteritems()]\n",
    "\n",
    "# WHY zscore is NaN - division by 0\n",
    "# sorted_counts_lit = sorted(country_dict['lithuania'], key=lambda x: x[0])\n",
    "# print stats.zscore([y for x,y in sorted_counts_lit])\n",
    "# print sorted_counts_lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends / Popularity based on query_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 17), (1, 18), (2, 6), (3, 9)]\n",
      "[(0, 3), (2, 1), (3, 1)]\n",
      "[(0, 48), (1, 46), (2, 15), (3, 56)]\n",
      "[(0, 6), (2, 1)]\n",
      "[(0, 22), (1, 43), (2, 17), (3, 19)]\n",
      "[(0, 2), (1, 6), (2, 1), (3, 3)]\n",
      "[(0, 2), (1, 3), (3, 5)]\n",
      "[(0, 16), (1, 31), (2, 11), (3, 30)]\n",
      "[(0, 75), (1, 85), (2, 42), (3, 72)]\n",
      "[(0, 4), (2, 3), (3, 1)]\n",
      "[(0, 58), (1, 53), (2, 12), (3, 54)]\n",
      "[(0, 58), (1, 129), (2, 56), (3, 77)]\n",
      "[(0, 2), (1, 5), (3, 2)]\n",
      "[(0, 2), (1, 1), (2, 1)]\n",
      "[(0, 4), (1, 15), (2, 26), (3, 5)]\n",
      "[(0, 3)]\n",
      "[(0, 557), (1, 827), (2, 379), (3, 450)]\n",
      "[(0, 52), (1, 93), (2, 28), (3, 73)]\n",
      "[(0, 83), (1, 104), (2, 33), (3, 63)]\n",
      "[(0, 14), (1, 14), (2, 4), (3, 17)]\n",
      "[(0, 1), (2, 1), (3, 1)]\n",
      "[(0, 3), (1, 4), (2, 2), (3, 9)]\n",
      "[(0, 4), (1, 10), (2, 8), (3, 9)]\n",
      "[(0, 8), (1, 10), (2, 1), (3, 10)]\n",
      "[(0, 17), (1, 16), (2, 7), (3, 16)]\n",
      "[(0, 2)]\n",
      "[(0, 2), (1, 2), (2, 3), (3, 6)]\n",
      "[(0, 413), (1, 375), (2, 165), (3, 269)]\n",
      "[(0, 1), (1, 1), (3, 4)]\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3)]\n",
      "[(0, 8)]\n",
      "[(0, 6), (1, 21), (2, 14), (3, 8)]\n",
      "[(0, 219), (1, 160), (2, 93), (3, 157)]\n",
      "[(0, 2), (1, 3), (2, 1)]\n",
      "[(0, 1)]\n",
      "[(0, 11), (1, 17), (2, 3), (3, 9)]\n",
      "[(0, 6), (1, 9), (2, 9), (3, 7)]\n",
      "[(0, 5), (1, 11), (2, 10), (3, 4)]\n",
      "[(0, 119), (1, 202), (2, 74), (3, 127)]\n",
      "[(0, 20), (1, 22), (2, 7), (3, 22)]\n",
      "[(0, 2), (2, 12), (3, 4)]\n",
      "[(0, 2), (1, 2), (3, 5)]\n",
      "[(0, 2)]\n",
      "[(0, 1), (1, 1), (2, 2), (3, 4)]\n",
      "[(0, 2), (2, 3)]\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4)]\n",
      "[(0, 25), (1, 46), (2, 21), (3, 41)]\n",
      "[(0, 16), (1, 20), (2, 10), (3, 10)]\n",
      "[(0, 137), (1, 251), (2, 71), (3, 97)]\n",
      "[(0, 18), (1, 33), (2, 13), (3, 52)]\n",
      "[(0, 3)]\n",
      "[(0, 1), (1, 4), (2, 1), (3, 1)]\n",
      "[(0, 1)]\n",
      "[(0, 18), (1, 2)]\n",
      "[(0, 68), (1, 97), (2, 41), (3, 75)]\n",
      "[(0, 1), (1, 8), (2, 6), (3, 1)]\n",
      "[(0, 190), (1, 217), (2, 111), (3, 178)]\n",
      "[(0, 6), (1, 10), (2, 6), (3, 4)]\n",
      "[(0, 9), (1, 30), (2, 15), (3, 12)]\n",
      "[(0, 33), (1, 53), (2, 48), (3, 79)]\n",
      "[(0, 5), (1, 7), (2, 14), (3, 8)]\n",
      "[(0, 2)]\n",
      "[(0, 9), (1, 9), (2, 2), (3, 9)]\n",
      "[(0, 186), (1, 248), (2, 110), (3, 221)]\n",
      "[(0, 14), (1, 28), (2, 6), (3, 6)]\n",
      "[(0, 3)]\n",
      "[(0, 532), (1, 794), (2, 362), (3, 567)]\n",
      "[(0, 4), (1, 2), (2, 3)]\n",
      "[(0, 106), (1, 122), (2, 39), (3, 117)]\n",
      "[(0, 1), (1, 5), (2, 6), (3, 2)]\n",
      "[(0, 1)]\n",
      "[(0, 6), (1, 11), (2, 5), (3, 7)]\n",
      "[(0, 16), (1, 28), (2, 8), (3, 21)]\n",
      "[(0, 57), (1, 67), (2, 52), (3, 70)]\n",
      "[(0, 39), (1, 67), (2, 30), (3, 57)]\n",
      "[(0, 1)]\n",
      "[(0, 2065), (1, 3349), (2, 1899), (3, 2351)]\n",
      "[(0, 2), (1, 12), (2, 8), (3, 1)]\n",
      "[(0, 4)]\n",
      "[(0, 236), (1, 295), (2, 175), (3, 303)]\n",
      "[(0, 282), (1, 440), (2, 256), (3, 601)]\n",
      "[(0, 58), (1, 46), (2, 17), (3, 23)]\n",
      "[(0, 1)]\n",
      "[(0, 1)]\n",
      "[(0, 22), (1, 32), (2, 9), (3, 20)]\n",
      "[(0, 18), (1, 42), (2, 5), (3, 30)]\n",
      "[(u'churros', -0.68313005106397329), (u'samosa', -0.70710678118654757), (u'sashimi', 0.94487725332567385), (u'spring_rolls', -1.0), (u'panna_cotta', -0.60088515490221606), (u'beef_tartare', 0.0), (u'cannoli', 0.6013958765199896), (u'foie_gras', 0.92069654023749758), (u'tacos', 0.21821789023599236), (u'pad_thai', -1.3363062095621219), (u'poutine', 0.52101988151831113), (u'ramen', -0.10185597561975659), (u'bibimbap', -0.70710678118654746), (u'beignets', -0.70710678118654735), (u'crab_cakes', nan), (u'apple_pie', -0.84248393445259562), (u'risotto', -0.60636564694109274), (u'paella', 0.47577156217586969), (u'steak', -0.29607176096386573), (u'baby_back_ribs', 0.9658242787731629), (u'miso_soup', nan), (u'frozen_yogurt', 1.671258043593467), (u'club_sandwich', 0.54882129994845175), (u'carrot_cake', 0.74331111623943458), (u'falafel', 0.4923659639173309), (u'bread_pudding', nan), (u'chicken_wings', 1.6774842736586515), (u'gnocchi', -0.37717662530875734), (u'caprese_salad', 1.4142135623730949), (u'creme_brulee', 1.507556722888818), (u'escargots', nan), (u'chocolate_cake', -0.72686751217532175), (u'tiramisu', -0.0056081624515270066), (u'spaghetti_bolognese', -1.2247448713915889), (u'mussels', 0.67891460425624095), (u'scallops', -0.20000000000000001), (u'baklava', -0.57735026918962573), (u'edamame', -1.150792911137501), (u'pancakes', -0.076154467365686762), (u'garlic_bread', nan), (u'onion_rings', -0.46291004988627571), (u'red_velvet_cake', 1.4142135623730949), (u'grilled_salmon', nan), (u'chicken_curry', nan), (u'deviled_eggs', 1.0), (u'caesar_salad', 0.47140452079103173), (u'hummus', 0.73830453775578853), (u'fish_and_chips', -0.94280904158206347), (u'lasagna', -0.61042900827572621), (u'guacamole', 1.5149293204592038), (u'strawberry_shortcake', nan), (u'croque_madame', -0.57735026918962573), (u'french_onion_soup', nan), (u'fried_rice', -1.0), (u'donuts', 0.2375928278323505), (u'gyoza', -0.97332852678457527), (u'ravioli', 0.10251356592513193), (u'french_toast', -1.1470786693528088), (u'ceviche', -0.55708601453115569), (u'bruschetta', 1.5522543371305446), (u'french_fries', -0.14907119849998599), (u'shrimp_and_grits', nan), (u'filet_mignon', 0.57735026918962573), (u'hamburger', 0.57427643856079535), (u'dumplings', -0.83462232611198583), (u'tuna_tartare', 1.6329931618554523), (u'sushi', 0.021118064679481133), (u'eggs_benedict', 0.0), (u'cup_cakes', 0.62847778482521466), (u'takoyaki', -0.72760687510899891), (u'chocolate_mousse', nan), (u'breakfast_burrito', -0.10976425998969035), (u'hot_dog', 0.37707514360508348), (u'macarons', 1.1648208067068038), (u'greek_salad', 1.3363062095621219), (u'huevos_rancheros', nan), (u'pizza', -0.11557477966587698), (u'pho', -1.0571882797418488), (u'prime_rib', nan), (u'cheesecake', 0.98424314728632423), (u'ice_cream', 1.4909024481263353), (u'omelette', -0.77898797302557365), (u'grilled_cheese_sandwich', nan), (u'lobster_roll_sandwich', nan), (u'nachos', -0.091841516238394039), (u'oysters', 0.45439526605480413)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foodmap/anaconda2/lib/python2.7/site-packages/scipy/stats/stats.py:2240: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (a - mns) / sstd\n"
     ]
    }
   ],
   "source": [
    "# get trends\n",
    "result = query_by_country(country)\n",
    "\n",
    "# this is dynamic but we can computer static totals and pass them as params\n",
    "def get_total_freq_category(category):\n",
    "    result_categ = query_by_category(category)\n",
    "    total = 0\n",
    "    for x in result_categ['hits']['hits']:\n",
    "        total += x[\"_source\"]['count']\n",
    "    return total\n",
    "\n",
    "def get_trend_per_category(query_result, analysis_type=\"trend\", interval=None):\n",
    "    category_dict = defaultdict(list)\n",
    "    for x in query_result['hits']['hits']:\n",
    "        category_dict[x[\"_source\"]['category']].append(tuple((x[\"_source\"]['date'], x[\"_source\"]['count'])))\n",
    "\n",
    "    category_trend = defaultdict(float)\n",
    "    for category, values in category_dict.iteritems():\n",
    "        # sort list by date ascending\n",
    "        sorted_counts = sorted(values, key=lambda x: x[0])\n",
    "        # we refer to the current day in the interval\n",
    "        if analysis_type == \"frequency\":\n",
    "            category_trend[category] = sum([y for x, y in sorted_counts])\n",
    "        elif analysis_type == \"relative_frequency\":\n",
    "            total_sum = get_total_freq_category(category)\n",
    "            category_trend[category] = sum([y for x,y in sorted_counts])/(1.0*total_sum)\n",
    "            print category, sum([y for x,y in sorted_counts]), total_sum\n",
    "        elif analysis_type == \"popularity\":\n",
    "            if interval is None:\n",
    "                y = [y for x, y in sorted_counts]\n",
    "            else:\n",
    "                new_intervals = split_intervat_in_buckets(interval, sorted_counts)\n",
    "                print new_intervals\n",
    "                y = [y for x, y in new_intervals]\n",
    "            category_trend[category] = stats.zscore(y)[-1]\n",
    "        elif analysis_type == \"trend\":\n",
    "            if interval is None:\n",
    "                y = [y for x, y in sorted_counts]\n",
    "            else:\n",
    "                new_intervals = split_intervat_in_buckets(interval, sorted_counts)\n",
    "                print new_intervals\n",
    "                y = [y for x, y in new_intervals]\n",
    "            x = np.arange(len(y))\n",
    "            try:\n",
    "                regression = np.polyfit(x, y, 1)\n",
    "                category_trend[category] = regression[0]\n",
    "            except:\n",
    "                # the for doesn't work for this example: np.polyfit(np.array([0]), np.array([1]), 1)\n",
    "                # some versions of numpy return nan, as should we\n",
    "                category_trend[category] = np.nan\n",
    "\n",
    "    return category_trend\n",
    "\n",
    "newd = get_trend_per_category(result, analysis_type=\"popularity\", interval=30)\n",
    "# newd = get_trend_per_category(result, analysis_type=\"popularity\")\n",
    "print [(x,y) for x,y in newd.iteritems()]\n",
    "\n",
    "# WHY zscore is NaN - division by 0\n",
    "# sorted_counts_lit = sorted(country_dict['lithuania'], key=lambda x: x[0])\n",
    "# print stats.zscore([y for x,y in sorted_counts_lit])\n",
    "# print sorted_counts_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = query_by_country(\"united states\")  # no results !!! very strange, we can see there are records in match_all\n",
    "# result = query_by_country(\"maldives\")  # no results\n",
    "first10 = [x for x in result['hits']['hits'][:10]]\n",
    "print first10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_total_freq_country(\"italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "def datetimetotimestamp(dt):\n",
    "    return (dt - datetime(1970, 1, 1)).total_seconds()\n",
    "\n",
    "def split_intervat_in_buckets(interval, daily_frequencies):    \n",
    "    last_ts = datetimetotimestamp(datetime.strptime(str(max(tpl[0] for tpl in daily_frequencies)), '%Y%m%d'))\n",
    "    \n",
    "    tsinterval = interval * 3600 * 24\n",
    "    interval_dict = Counter()\n",
    "    for x, freq in daily_frequencies:\n",
    "        ts = datetimetotimestamp(datetime.strptime(str(x), '%Y%m%d'))\n",
    "        \n",
    "        tsid = int((ts - last_ts) / tsinterval)\n",
    "        interval_dict[tsid] += freq\n",
    "    \n",
    "    minkey = min(interval_dict.keys())\n",
    "    return sorted([(tsid-minkey, freq) for tsid, freq in interval_dict.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20170310, 3), (20170311, 21), (20170312, 7), (20170313, 10), (20170314, 17), (20170315, 18), (20170316, 21), (20170317, 11), (20170318, 18), (20170319, 8), (20170320, 27), (20170321, 15), (20170322, 13), (20170323, 10), (20170324, 8), (20170325, 9), (20170326, 13), (20170327, 13), (20170328, 10), (20170329, 17), (20170330, 20), (20170331, 12), (20170401, 10), (20170402, 14), (20170403, 9), (20170404, 23), (20170405, 18), (20170406, 11), (20170407, 11), (20170408, 6), (20170409, 12), (20170410, 16), (20170411, 7), (20170412, 8), (20170413, 12), (20170414, 10), (20170415, 26), (20170416, 8), (20170417, 40), (20170418, 17), (20170419, 17), (20170420, 20), (20170421, 11), (20170422, 15), (20170423, 10), (20170424, 10), (20170425, 13), (20170426, 6), (20170427, 19), (20170428, 21), (20170429, 13), (20170430, 23), (20170501, 8), (20170502, 62), (20170503, 21), (20170504, 17), (20170505, 85), (20170506, 17), (20170507, 19), (20170508, 12), (20170509, 8), (20170510, 17), (20170511, 10), (20170512, 26), (20170513, 2), (20170531, 1), (20170601, 11), (20170602, 20), (20170603, 14), (20170604, 9), (20170605, 8), (20170606, 18), (20170607, 94), (20170608, 11), (20170609, 6), (20170610, 10), (20170611, 13), (20170612, 9), (20170613, 9), (20170614, 7), (20170615, 14), (20170616, 5), (20170617, 9), (20170618, 4), (20170619, 14), (20170620, 25), (20170621, 10), (20170622, 13), (20170623, 9), (20170624, 21), (20170625, 11), (20170626, 7), (20170627, 11)]\n",
      "[(0, 58), (1, 118), (2, 76), (3, 105), (4, 81), (5, 121), (6, 96), (7, 152), (8, 179), (9, 55), (12, 81), (13, 152), (14, 78), (15, 82)]\n",
      "Freq before intervals churros 1434\n",
      "Freq after intervals churros 1434\n",
      "Last day popularity churros churros -0.325015249788\n"
     ]
    }
   ],
   "source": [
    "query_result = query_by_country(\"united states\")\n",
    "\n",
    "category_dict = defaultdict(list)\n",
    "for x in query_result['hits']['hits']:\n",
    "    category_dict[x[\"_source\"]['category']].append(tuple((x[\"_source\"]['date'], x[\"_source\"]['count'])))\n",
    "    \n",
    "for category, values in category_dict.iteritems():\n",
    "    sorted_counts = sorted(values, key=lambda x: x[0])\n",
    "    print sorted_counts\n",
    "    new_intervals = split_intervat_in_buckets(7, sorted_counts)\n",
    "    print new_intervals\n",
    "    print \"Freq before intervals\", category, sum([y for x, y in sorted_counts])\n",
    "    print \"Freq after intervals\", category, sum([y for x, y in new_intervals])\n",
    "    print \"Last day popularity\", category, category, stats.zscore([y for x, y in sorted_counts])[-1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRENDS\n",
    "Idea: plot trend line on scatter/line plots with daily values\n",
    "\n",
    "https://docs.scipy.org/doc/numpy-1.3.x/reference/generated/numpy.polyfit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091, 0.1954,  0.6307,  0.6599,  0.1065,  0.0508]) \n",
    "x = np.arange(len(y))\n",
    "\n",
    "regression = np.polyfit(x, y, 1)\n",
    "regression\n",
    "# y = regression[0] * x + regression[1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(y)\n",
    "new_y = [regression[0]*i+regression[1] for i in x]\n",
    "plt.plot(new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZSCORE\n",
    "Calculates the z score of each value in the sample, relative to the sample mean and standard deviation.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.zscore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091, 0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n",
    "stats.zscore(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for vini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "os.chdir(\"/home/foodmap/food101/\")\n",
    "from processing.location.locations import Countries\n",
    "\n",
    "def getCountriesAsList():\n",
    "    countriesIndex, countriesInfo = Countries.loadFromFile()\n",
    "    country_list = list()\n",
    "    for k,v in countriesInfo.iteritems():\n",
    "        country_list.append(v[0].encode('ascii','ignore'))\n",
    "    country_list = sorted(country_list)\n",
    "    return country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = getCountriesAsList()\n",
    "print len(a), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst = [ x.strip().replace(' ', '_').lower() for x in categories]\n",
    "list(set(lst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
